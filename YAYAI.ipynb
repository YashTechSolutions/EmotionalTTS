{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n82ck7jbMc0X"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, RobertaTokenizer, RobertaForSequenceClassification\n",
        "from torch.optim import AdamW\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import seaborn as sns\n",
        "import nltk\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "from collections import defaultdict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DNcxk-nUMfh-"
      },
      "outputs": [],
      "source": [
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Load datasets\n",
        "df = pd.read_csv('/content/drive/MyDrive/TTS/combined_emotion.csv')\n",
        "df.columns = ['text', 'emotion']\n",
        "\n",
        "df2 = pd.read_csv('/content/drive/MyDrive/TTS/combined_sentiment_data.csv')\n",
        "df2.columns = ['text', 'sentiment']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m_GNyJ-rMmIu"
      },
      "outputs": [],
      "source": [
        "# Preprocessing\n",
        "print(\"Missing values:\")\n",
        "print(df.isnull().sum())\n",
        "df = df.dropna()\n",
        "\n",
        "print(\"Missing values:\")\n",
        "print(df2.isnull().sum())\n",
        "df2 = df2.dropna()\n",
        "\n",
        "# Plot emotion and sentiment distributions\n",
        "plt.figure(figsize=(10,5))\n",
        "sns.countplot(data=df, x='emotion', order=df['emotion'].value_counts().index)\n",
        "plt.xticks(rotation=45)\n",
        "plt.title(\"Emotion Distribution\")\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "sns.countplot(data=df2, x='sentiment', order=df2['sentiment'].value_counts().index)\n",
        "plt.xticks(rotation=45)\n",
        "plt.title(\"Sentiment Distribution\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# VADER word-level sentiment scoring\n",
        "nltk.download('vader_lexicon')\n",
        "sid = SentimentIntensityAnalyzer()\n",
        "\n",
        "# Define and apply Singlish sentiment lexicon\n",
        "singlish_lexicon = {\n",
        "    \"shiok\": 1.2, \"sian\": -1.0, \"aiyah\": -0.3, \"aiyo\": -0.6, \"sabo\": -1.2,\n",
        "    \"steady\": 0.8, \"steady lah\": 1.0, \"relac\": 0.6, \"chiong\": 0.3, \"lepak\": 0.7,\n",
        "    \"ang moh\": 0.0, \"chio\": 0.9, \"makan\": 0.3, \"bo jio\": -0.6, \"paiseh\": -0.2,\n",
        "    \"alamak\": -0.8, \"wah lao\": -0.6, \"wah piang\": -0.6, \"lah\": 0.1, \"leh\": 0.1,\n",
        "    \"lor\": 0.05, \"meh\": -0.1, \"hor\": 0.1, \"can lah\": 0.5, \"no need lah\": -0.3,\n",
        "    \"okay lah\": 0.5, \"donâ€™t play play\": 0.3, \"jialat\": -1.2, \"jialat sia\": -1.5,\n",
        "    \"heng ah\": 0.8, \"wah so good\": 1.0, \"wah so bad\": -1.0, \"buay tahan\": -0.9,\n",
        "    \"shiok sia\": 1.0, \"win liao lor\": 1.0, \"confirm plus chop\": 0.9, \"bo chup\": -0.4,\n",
        "    \"act blur\": -0.6, \"blur like sotong\": -0.9, \"lim kopi\": 0.5, \"lim teh\": 0.3,\n",
        "    \"catch no ball\": -0.3, \"donâ€™t anyhow\": -0.3, \"so stress sia\": -1.2,\n",
        "    \"ownself check ownself\": -0.5, \"talk cock\": -0.6, \"chio bu\": 0.9,\n",
        "    \"ah beng\": -0.3, \"ah lian\": -0.3, \"guai kia\": 0.6, \"kancheong spider\": -0.5,\n",
        "    \"spoil market\": -0.4, \"last warning ah\": -0.8\n",
        "}\n",
        "sid.lexicon.update(singlish_lexicon)\n",
        "\n",
        "# Save the Singlish lexicon to CSV for future updating\n",
        "singlish_df = pd.DataFrame(list(singlish_lexicon.items()), columns=['word', 'avg_sentiment_score'])\n",
        "singlish_df.to_csv('/content/drive/MyDrive/TTS/singlish_sentiment_lexicon.csv', index=False)\n",
        "\n",
        "# Containers for scores and word counts\n",
        "word_scores = defaultdict(float)\n",
        "word_counts = defaultdict(int)\n",
        "\n",
        "for text in df2['text']:\n",
        "    words = text.split()\n",
        "    for word in words:\n",
        "        score = sid.polarity_scores(word)['compound']\n",
        "        word_scores[word] += score\n",
        "        word_counts[word] += 1\n",
        "\n",
        "average_word_scores = {word: word_scores[word] / word_counts[word] for word in word_scores}\n",
        "word_sentiment_df = pd.DataFrame.from_dict(average_word_scores, orient='index', columns=['avg_sentiment_score'])\n",
        "word_sentiment_df = word_sentiment_df.sort_values(by='avg_sentiment_score', ascending=False)\n",
        "word_sentiment_df.to_csv('/content/drive/MyDrive/TTS/word_sentiment_scores.csv')\n",
        "\n",
        "print(\"Top Positive Words:\")\n",
        "print(word_sentiment_df.head(10))\n",
        "print(\"\\nTop Negative Words:\")\n",
        "print(word_sentiment_df.tail(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iMuIxKomMpPv"
      },
      "outputs": [],
      "source": [
        "def get_sentiment_label(text):\n",
        "    score = sid.polarity_scores(text)['compound']\n",
        "    if score >= 0.05:\n",
        "        return 2  # positive\n",
        "    elif score <= -0.05:\n",
        "        return 0  # negative\n",
        "    else:\n",
        "        return 1  # neutral\n",
        "\n",
        "# Encode labels\n",
        "label_mapping_emotion = {label: idx for idx, label in enumerate(df['emotion'].unique())}\n",
        "df['emotion'] = df['emotion'].map(label_mapping_emotion)\n",
        "\n",
        "#label_mapping_sentiment = {label: idx for idx, label in enumerate(df2['sentiment'].unique())}\n",
        "#df2['sentiment'] = df2['sentiment'].map(label_mapping_sentiment)\n",
        "\n",
        "df2['sentiment'] = df2['text'].apply(get_sentiment_label)\n",
        "\n",
        "# Split data\n",
        "min_len = min(len(df), len(df2))  # Get the minimum size of the two datasets\n",
        "df = df[:min_len]\n",
        "df2 = df2[:min_len]\n",
        "\n",
        "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
        "    df['text'].tolist(), df['emotion'].tolist(), test_size=0.2, random_state=42, stratify=df['emotion']\n",
        ")\n",
        "\n",
        "val_texts, test_texts, val_labels, test_labels = train_test_split(\n",
        "    test_texts, test_labels, test_size=0.1, random_state=42, stratify=test_labels\n",
        ")\n",
        "\n",
        "train_texts_2, test_texts_2, train_labels_2, test_labels_2 = train_test_split(\n",
        "    df2['text'].tolist(), df2['sentiment'].tolist(), test_size=0.2, random_state=42, stratify=df2['sentiment']\n",
        ")\n",
        "\n",
        "val_texts_2, test_texts_2, val_labels_2, test_labels_2 = train_test_split(\n",
        "    test_texts_2, test_labels_2, test_size=0.1, random_state=42, stratify=test_labels_2\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QzTr8MfYMs8_"
      },
      "outputs": [],
      "source": [
        "# Tokenizers\n",
        "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "roberta_tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "\n",
        "# Encode texts\n",
        "def encode_texts(texts, tokenizer, max_length=128):\n",
        "    return tokenizer(texts, padding=True, truncation=True, max_length=max_length, return_tensors=\"pt\")\n",
        "\n",
        "train_encodings_bert = encode_texts(train_texts, bert_tokenizer)\n",
        "val_encodings_bert = encode_texts(val_texts, bert_tokenizer)\n",
        "train_encodings_roberta = encode_texts(train_texts_2, roberta_tokenizer)\n",
        "val_encodings_roberta = encode_texts(val_texts_2, roberta_tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Re-encode using the same tokenizer\n",
        "train_encodings_roberta2 = encode_texts(train_texts_2, roberta_tokenizer)\n",
        "val_encodings_roberta2 = encode_texts(val_texts_2, roberta_tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Dataset class\n",
        "class SentimentDataset(Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
        "        item[\"labels\"] = torch.tensor(self.labels[idx])\n",
        "        return item"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Datasets and loaders\n",
        "train_dataset_roberta2 = SentimentDataset(train_encodings_roberta2, train_labels_2)\n",
        "val_dataset_roberta2 = SentimentDataset(val_encodings_roberta2, val_labels_2)\n",
        "\n",
        "train_loader_roberta2 = DataLoader(train_dataset_roberta2, batch_size=16, shuffle=True)\n",
        "val_loader_roberta2 = DataLoader(val_dataset_roberta2, batch_size=16, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "roberta_model2 = RobertaForSequenceClassification.from_pretrained(\n",
        "    'roberta-base',\n",
        "    num_labels=3\n",
        ")\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "roberta_model2.to(device)\n",
        "optimizer2 = AdamW(roberta_model2.parameters(), lr=2e-5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_roberta_model(model, loader, optimizer, epochs=3):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for batch in tqdm(loader):\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            loss = outputs.loss\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            preds = torch.argmax(outputs.logits, dim=1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "        accuracy = correct / total\n",
        "        print(f\"Epoch {epoch+1}: Loss = {total_loss/len(loader):.4f}, Accuracy = {accuracy:.4f}\")\n",
        "\n",
        "def evaluate_roberta_model(model, loader):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in loader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            outputs = model(input_ids, attention_mask=attention_mask)\n",
        "            preds = torch.argmax(outputs.logits, dim=1)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    target_names = [\"negative\", \"neutral\", \"positive\"]\n",
        "    print(\"Classification Report:\")\n",
        "    print(classification_report(all_labels, all_preds, target_names=target_names))\n",
        "\n",
        "    cm = confusion_matrix(all_labels, all_preds)\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=target_names, yticklabels=target_names)\n",
        "    plt.xlabel(\"Predicted\")\n",
        "    plt.ylabel(\"Actual\")\n",
        "    plt.title(\"Sentiment Classification - Confusion Matrix\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train and evaluate\n",
        "train_roberta_model(roberta_model2, train_loader_roberta2, optimizer2, epochs=8)\n",
        "evaluate_roberta_model(roberta_model2, val_loader_roberta2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m9sSelTqMwTV"
      },
      "outputs": [],
      "source": [
        "# Create multitask dataset\n",
        "class MultiTaskDataset(Dataset):\n",
        "    def __init__(self, encodings_bert, labels_emotion, encodings_roberta, labels_sentiment):\n",
        "        self.encodings_bert = encodings_bert\n",
        "        self.labels_emotion = labels_emotion\n",
        "        self.encodings_roberta = encodings_roberta\n",
        "        self.labels_sentiment = labels_sentiment\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels_emotion)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {\n",
        "            'input_ids_bert': torch.tensor(self.encodings_bert['input_ids'][idx]),\n",
        "            'attention_mask_bert': torch.tensor(self.encodings_bert['attention_mask'][idx]),\n",
        "            'labels_emotion': torch.tensor(self.labels_emotion[idx]),\n",
        "            'input_ids_roberta': torch.tensor(self.encodings_roberta['input_ids'][idx]),\n",
        "            'attention_mask_roberta': torch.tensor(self.encodings_roberta['attention_mask'][idx]),\n",
        "            'labels_sentiment': torch.tensor(self.labels_sentiment[idx]),\n",
        "        }\n",
        "        return item"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LKRPOE5sMx-e"
      },
      "outputs": [],
      "source": [
        "# Create DataLoader\n",
        "train_dataset = MultiTaskDataset(train_encodings_bert, train_labels, train_encodings_roberta, train_labels_2)\n",
        "val_dataset = MultiTaskDataset(val_encodings_bert, val_labels, val_encodings_roberta, val_labels_2)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
        "\n",
        "# Define models\n",
        "bert_model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(label_mapping_emotion))\n",
        "roberta_model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=3)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "bert_model.to(device)\n",
        "roberta_model.to(device)\n",
        "\n",
        "optimizer = AdamW(list(bert_model.parameters()) + list(roberta_model.parameters()), lr=2e-5)\n",
        "loss_fn = CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IKBixh1SM1vI"
      },
      "outputs": [],
      "source": [
        "# Training loop\n",
        "def train_model(bert_model, roberta_model, train_loader, val_loader, epochs=8):\n",
        "    bert_model.train()\n",
        "    roberta_model.train()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        print(f\"Epoch {epoch+1}\")\n",
        "        total_loss = 0\n",
        "        correct_bert = 0\n",
        "        total_bert = 0\n",
        "        correct_roberta = 0\n",
        "        total_roberta = 0\n",
        "\n",
        "        for batch in tqdm(train_loader):\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # BERT part\n",
        "            input_ids_bert = batch['input_ids_bert'].to(device)\n",
        "            attention_mask_bert = batch['attention_mask_bert'].to(device)\n",
        "            labels_emotion = batch['labels_emotion'].to(device)\n",
        "            outputs_bert = bert_model(input_ids_bert, attention_mask=attention_mask_bert, labels=labels_emotion)\n",
        "            loss_bert = outputs_bert.loss\n",
        "            total_loss += loss_bert.item()\n",
        "            loss_bert.backward()\n",
        "\n",
        "            preds_bert = torch.argmax(outputs_bert.logits, dim=1)\n",
        "            correct_bert += (preds_bert == labels_emotion).sum().item()\n",
        "            total_bert += labels_emotion.size(0)\n",
        "\n",
        "            # RoBERTa part\n",
        "            input_ids_roberta = batch['input_ids_roberta'].to(device)\n",
        "            attention_mask_roberta = batch['attention_mask_roberta'].to(device)\n",
        "            labels_sentiment = batch['labels_sentiment'].to(device)\n",
        "            outputs_roberta = roberta_model(input_ids_roberta, attention_mask=attention_mask_roberta, labels=labels_sentiment)\n",
        "            loss_roberta = outputs_roberta.loss\n",
        "            total_loss += loss_roberta.item()\n",
        "            loss_roberta.backward()\n",
        "\n",
        "            preds_roberta = torch.argmax(outputs_roberta.logits, dim=1)\n",
        "            correct_roberta += (preds_roberta == labels_sentiment).sum().item()\n",
        "            total_roberta += labels_sentiment.size(0)\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "        accuracy_bert = correct_bert / total_bert\n",
        "        accuracy_roberta = correct_roberta / total_roberta\n",
        "        print(f\"Training Loss: {total_loss / len(train_loader)} | BERT Accuracy: {accuracy_bert:.4f} | RoBERTa Accuracy: {accuracy_roberta:.4f}\")\n",
        "\n",
        "# Train the model\n",
        "train_model(bert_model, roberta_model, train_loader, val_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e7TVPa_vM4DQ"
      },
      "outputs": [],
      "source": [
        "# Save the models\n",
        "bert_model.save_pretrained(\"emotion_bert_model\")\n",
        "roberta_model.save_pretrained(\"sentiment_roberta_model\")\n",
        "bert_tokenizer.save_pretrained(\"emotion_bert_model\")\n",
        "roberta_tokenizer.save_pretrained(\"sentiment_roberta_model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i_fa6IB0M5j9"
      },
      "outputs": [],
      "source": [
        "# Evaluation function\n",
        "def evaluate_model(bert_model, roberta_model, val_loader):\n",
        "    bert_model.eval()\n",
        "    roberta_model.eval()\n",
        "\n",
        "    all_preds_bert, all_labels_bert = [], []\n",
        "    all_preds_roberta, all_labels_roberta = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(val_loader, desc=\"Evaluating\"):\n",
        "            # BERT part\n",
        "            input_ids_bert = batch['input_ids_bert'].to(device)\n",
        "            attention_mask_bert = batch['attention_mask_bert'].to(device)\n",
        "            labels_emotion = batch['labels_emotion'].to(device)\n",
        "            outputs_bert = bert_model(input_ids_bert, attention_mask=attention_mask_bert)\n",
        "            preds_bert = torch.argmax(outputs_bert.logits, dim=1)\n",
        "            all_preds_bert.extend(preds_bert.cpu().numpy())\n",
        "            all_labels_bert.extend(labels_emotion.cpu().numpy())\n",
        "\n",
        "            # RoBERTa part\n",
        "            input_ids_roberta = batch['input_ids_roberta'].to(device)\n",
        "            attention_mask_roberta = batch['attention_mask_roberta'].to(device)\n",
        "            labels_sentiment = batch['labels_sentiment'].to(device)\n",
        "            outputs_roberta = roberta_model(input_ids_roberta, attention_mask=attention_mask_roberta)\n",
        "            preds_roberta = torch.argmax(outputs_roberta.logits, dim=1)\n",
        "            all_preds_roberta.extend(preds_roberta.cpu().numpy())\n",
        "            all_labels_roberta.extend(labels_sentiment.cpu().numpy())\n",
        "\n",
        "    # Emotion labels\n",
        "    try:\n",
        "      target_names_emotion = [label for label, idx in sorted(label_mapping_emotion.items(), key=lambda x: x[1])]\n",
        "      target_names_emotion = [str(label) for label in target_names_emotion]\n",
        "    except:\n",
        "      target_names_emotion = [str(label) for label in sorted(set(all_labels_bert))]\n",
        "\n",
        "    target_names_sentiment = [\"negative\", \"neutral\", \"positive\"]\n",
        "\n",
        "    print(\"\\nðŸ“Š BERT Classification Report (Emotion):\")\n",
        "    print(classification_report(all_labels_bert, all_preds_bert, target_names=target_names_emotion))\n",
        "\n",
        "    print(\"\\nðŸ“Š RoBERTa Classification Report (Sentiment):\")\n",
        "    print(classification_report(all_labels_roberta, all_preds_roberta, target_names=target_names_sentiment))\n",
        "\n",
        "    # Confusion Matrix for BERT\n",
        "    cm_bert = confusion_matrix(all_labels_bert, all_preds_bert)\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(cm_bert, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=target_names_emotion, yticklabels=target_names_emotion)\n",
        "    plt.xlabel(\"Predicted\")\n",
        "    plt.ylabel(\"Actual\")\n",
        "    plt.title(\"Emotion Classification - Confusion Matrix\")\n",
        "    plt.show()\n",
        "\n",
        "    # Confusion Matrix for RoBERTa\n",
        "    cm_roberta = confusion_matrix(all_labels_roberta, all_preds_roberta)\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(cm_roberta, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=target_names_sentiment, yticklabels=target_names_sentiment)\n",
        "    plt.xlabel(\"Predicted\")\n",
        "    plt.ylabel(\"Actual\")\n",
        "    plt.title(\"Sentiment Classification - Confusion Matrix\")\n",
        "    plt.show()\n",
        "\n",
        "# Evaluate the model\n",
        "evaluate_model(bert_model, roberta_model, val_loader)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "ai_project",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.21"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
