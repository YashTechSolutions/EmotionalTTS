{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n82ck7jbMc0X"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, RobertaTokenizer, RobertaForSequenceClassification\n",
        "from torch.optim import AdamW\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Load datasets\n",
        "df = pd.read_csv('/content/drive/MyDrive/TTS/combined_emotion.csv')\n",
        "df.columns = ['text', 'emotion']\n",
        "\n",
        "df2 = pd.read_csv('/content/drive/MyDrive/TTS/combined_sentiment_data.csv')\n",
        "df2.columns = ['text', 'sentiment']\n"
      ],
      "metadata": {
        "id": "DNcxk-nUMfh-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing\n",
        "print(\"Missing values:\")\n",
        "print(df.isnull().sum())\n",
        "df = df.dropna()\n",
        "\n",
        "print(\"Missing values:\")\n",
        "print(df2.isnull().sum())\n",
        "df2 = df2.dropna()\n",
        "\n",
        "# Plot emotion and sentiment distributions\n",
        "plt.figure(figsize=(10,5))\n",
        "sns.countplot(data=df, x='emotion', order=df['emotion'].value_counts().index)\n",
        "plt.xticks(rotation=45)\n",
        "plt.title(\"Emotion Distribution\")\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "sns.countplot(data=df2, x='sentiment', order=df2['sentiment'].value_counts().index)\n",
        "plt.xticks(rotation=45)\n",
        "plt.title(\"Sentiment Distribution\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "m_GNyJ-rMmIu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode labels\n",
        "label_mapping_emotion = {label: idx for idx, label in enumerate(df['emotion'].unique())}\n",
        "df['emotion'] = df['emotion'].map(label_mapping_emotion)\n",
        "\n",
        "label_mapping_sentiment = {label: idx for idx, label in enumerate(df2['sentiment'].unique())}\n",
        "df2['sentiment'] = df2['sentiment'].map(label_mapping_sentiment)\n",
        "\n",
        "# Split data\n",
        "min_len = min(len(df), len(df2))  # Get the minimum size of the two datasets\n",
        "df = df[:min_len]\n",
        "df2 = df2[:min_len]\n",
        "\n",
        "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
        "    df['text'].tolist(), df['emotion'].tolist(), test_size=0.2, random_state=42, stratify=df['emotion']\n",
        ")\n",
        "\n",
        "val_texts, test_texts, val_labels, test_labels = train_test_split(\n",
        "    test_texts, test_labels, test_size=0.1, random_state=42, stratify=test_labels\n",
        ")\n",
        "\n",
        "train_texts_2, test_texts_2, train_labels_2, test_labels_2 = train_test_split(\n",
        "    df2['text'].tolist(), df2['sentiment'].tolist(), test_size=0.2, random_state=42, stratify=df2['sentiment']\n",
        ")\n",
        "\n",
        "val_texts_2, test_texts_2, val_labels_2, test_labels_2 = train_test_split(\n",
        "    test_texts_2, test_labels_2, test_size=0.1, random_state=42, stratify=test_labels_2\n",
        ")\n"
      ],
      "metadata": {
        "id": "iMuIxKomMpPv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenizers\n",
        "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "roberta_tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "\n",
        "# Encode texts\n",
        "def encode_texts(texts, tokenizer, max_length=128):\n",
        "    return tokenizer(texts, padding=True, truncation=True, max_length=max_length, return_tensors=\"pt\")\n",
        "\n",
        "train_encodings_bert = encode_texts(train_texts, bert_tokenizer)\n",
        "val_encodings_bert = encode_texts(val_texts, bert_tokenizer)\n",
        "train_encodings_roberta = encode_texts(train_texts_2, roberta_tokenizer)\n",
        "val_encodings_roberta = encode_texts(val_texts_2, roberta_tokenizer)\n"
      ],
      "metadata": {
        "id": "QzTr8MfYMs8_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Create multitask dataset\n",
        "class MultiTaskDataset(Dataset):\n",
        "    def __init__(self, encodings_bert, labels_emotion, encodings_roberta, labels_sentiment):\n",
        "        self.encodings_bert = encodings_bert\n",
        "        self.labels_emotion = labels_emotion\n",
        "        self.encodings_roberta = encodings_roberta\n",
        "        self.labels_sentiment = labels_sentiment\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels_emotion)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {\n",
        "            'input_ids_bert': torch.tensor(self.encodings_bert['input_ids'][idx]),\n",
        "            'attention_mask_bert': torch.tensor(self.encodings_bert['attention_mask'][idx]),\n",
        "            'labels_emotion': torch.tensor(self.labels_emotion[idx]),\n",
        "            'input_ids_roberta': torch.tensor(self.encodings_roberta['input_ids'][idx]),\n",
        "            'attention_mask_roberta': torch.tensor(self.encodings_roberta['attention_mask'][idx]),\n",
        "            'labels_sentiment': torch.tensor(self.labels_sentiment[idx]),\n",
        "        }\n",
        "        return item\n"
      ],
      "metadata": {
        "id": "m9sSelTqMwTV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create DataLoader\n",
        "train_dataset = MultiTaskDataset(train_encodings_bert, train_labels, train_encodings_roberta, train_labels_2)\n",
        "val_dataset = MultiTaskDataset(val_encodings_bert, val_labels, val_encodings_roberta, val_labels_2)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
        "\n",
        "# Define models\n",
        "bert_model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(label_mapping_emotion))\n",
        "roberta_model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=len(label_mapping_sentiment))\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "bert_model.to(device)\n",
        "roberta_model.to(device)\n",
        "\n",
        "optimizer = AdamW(list(bert_model.parameters()) + list(roberta_model.parameters()), lr=2e-5)\n",
        "loss_fn = CrossEntropyLoss()\n"
      ],
      "metadata": {
        "id": "LKRPOE5sMx-e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Training loop\n",
        "def train_model(bert_model, roberta_model, train_loader, val_loader, epochs=3):\n",
        "    bert_model.train()\n",
        "    roberta_model.train()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        print(f\"Epoch {epoch+1}\")\n",
        "        total_loss = 0\n",
        "        correct_bert = 0\n",
        "        total_bert = 0\n",
        "        correct_roberta = 0\n",
        "        total_roberta = 0\n",
        "\n",
        "        for batch in tqdm(train_loader):\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # BERT part\n",
        "            input_ids_bert = batch['input_ids_bert'].to(device)\n",
        "            attention_mask_bert = batch['attention_mask_bert'].to(device)\n",
        "            labels_emotion = batch['labels_emotion'].to(device)\n",
        "            outputs_bert = bert_model(input_ids_bert, attention_mask=attention_mask_bert, labels=labels_emotion)\n",
        "            loss_bert = outputs_bert.loss\n",
        "            total_loss += loss_bert.item()\n",
        "            loss_bert.backward()\n",
        "\n",
        "            preds_bert = torch.argmax(outputs_bert.logits, dim=1)\n",
        "            correct_bert += (preds_bert == labels_emotion).sum().item()\n",
        "            total_bert += labels_emotion.size(0)\n",
        "\n",
        "            # RoBERTa part\n",
        "            input_ids_roberta = batch['input_ids_roberta'].to(device)\n",
        "            attention_mask_roberta = batch['attention_mask_roberta'].to(device)\n",
        "            labels_sentiment = batch['labels_sentiment'].to(device)\n",
        "            outputs_roberta = roberta_model(input_ids_roberta, attention_mask=attention_mask_roberta, labels=labels_sentiment)\n",
        "            loss_roberta = outputs_roberta.loss\n",
        "            total_loss += loss_roberta.item()\n",
        "            loss_roberta.backward()\n",
        "\n",
        "            preds_roberta = torch.argmax(outputs_roberta.logits, dim=1)\n",
        "            correct_roberta += (preds_roberta == labels_sentiment).sum().item()\n",
        "            total_roberta += labels_sentiment.size(0)\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "        accuracy_bert = correct_bert / total_bert\n",
        "        accuracy_roberta = correct_roberta / total_roberta\n",
        "        print(f\"Training Loss: {total_loss / len(train_loader)} | BERT Accuracy: {accuracy_bert:.4f} | RoBERTa Accuracy: {accuracy_roberta:.4f}\")\n",
        "\n",
        "# Train the model\n",
        "train_model(bert_model, roberta_model, train_loader, val_loader)\n"
      ],
      "metadata": {
        "id": "IKBixh1SM1vI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Save the models\n",
        "bert_model.save_pretrained(\"emotion_bert_model\")\n",
        "roberta_model.save_pretrained(\"sentiment_roberta_model\")\n",
        "bert_tokenizer.save_pretrained(\"emotion_bert_model\")\n",
        "roberta_tokenizer.save_pretrained(\"sentiment_roberta_model\")\n"
      ],
      "metadata": {
        "id": "e7TVPa_vM4DQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Evaluation function\n",
        "\n",
        "\n",
        "def evaluate_model(bert_model, roberta_model, val_loader):\n",
        "    bert_model.eval()\n",
        "    roberta_model.eval()\n",
        "\n",
        "    all_preds_bert, all_labels_bert = [], []\n",
        "    all_preds_roberta, all_labels_roberta = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            # BERT part\n",
        "            input_ids_bert = batch['input_ids_bert'].to(device)\n",
        "            attention_mask_bert = batch['attention_mask_bert'].to(device)\n",
        "            labels_emotion = batch['labels_emotion'].to(device)\n",
        "            outputs_bert = bert_model(input_ids_bert, attention_mask=attention_mask_bert)\n",
        "            preds_bert = torch.argmax(outputs_bert.logits, dim=1)\n",
        "            all_preds_bert.extend(preds_bert.cpu().numpy())\n",
        "            all_labels_bert.extend(labels_emotion.cpu().numpy())\n",
        "\n",
        "            # RoBERTa part\n",
        "            input_ids_roberta = batch['input_ids_roberta'].to(device)\n",
        "            attention_mask_roberta = batch['attention_mask_roberta'].to(device)\n",
        "            labels_sentiment = batch['labels_sentiment'].to(device)\n",
        "            outputs_roberta = roberta_model(input_ids_roberta, attention_mask=attention_mask_roberta)\n",
        "            preds_roberta = torch.argmax(outputs_roberta.logits, dim=1)\n",
        "            all_preds_roberta.extend(preds_roberta.cpu().numpy())\n",
        "            all_labels_roberta.extend(labels_sentiment.cpu().numpy())\n",
        "\n",
        "    print(\"BERT Classification Report:\")\n",
        "    print(classification_report(all_labels_bert, all_preds_bert, target_names=label_mapping_emotion.keys()))\n",
        "\n",
        "    print(\"RoBERTa Classification Report:\")\n",
        "    print(classification_report(all_labels_roberta, all_preds_roberta, target_names=label_mapping_sentiment.keys()))\n",
        "\n",
        "    # Confusion Matrix for BERT\n",
        "    cm_bert = confusion_matrix(all_labels_bert, all_preds_bert)\n",
        "    plt.figure(figsize=(8,6))\n",
        "    sns.heatmap(cm_bert, annot=True, fmt='d', cmap='Blues', xticklabels=label_mapping_emotion.keys(), yticklabels=label_mapping_emotion.keys())\n",
        "    plt.xlabel(\"Predicted\")\n",
        "    plt.ylabel(\"Actual\")\n",
        "    plt.title(\"Emotion Classification - Confusion Matrix\")\n",
        "    plt.show()\n",
        "\n",
        "    # Confusion Matrix for RoBERTa\n",
        "    cm_roberta = confusion_matrix(all_labels_roberta, all_preds_roberta)\n",
        "    plt.figure(figsize=(8,6))\n",
        "    sns.heatmap(cm_roberta, annot=True, fmt='d', cmap='Blues', xticklabels=label_mapping_sentiment.keys(), yticklabels=label_mapping_sentiment.keys())\n",
        "    plt.xlabel(\"Predicted\")\n",
        "    plt.ylabel(\"Actual\")\n",
        "    plt.title(\"Sentiment Classification - Confusion Matrix\")\n",
        "    plt.show()\n",
        "\n",
        "# Evaluate the model\n",
        "evaluate_model(bert_model, roberta_model, val_loader)\n"
      ],
      "metadata": {
        "id": "i_fa6IB0M5j9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}